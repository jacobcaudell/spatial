<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Gasson Hall: Photogrammetry vs Neural Reconstruction</title>
  <link rel="icon" type="image/png" href="assets/CSI_logo.png" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Outfit:wght@200;300;400;500;600;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header>
    <img src="assets/CSI_logo.png" alt="Caudell Spatial Logo" class="logo" />
    <h1>Gasson Hall: Photogrammetry vs Neural Reconstruction</h1>
  </header>

  <section class="video-gallery">
    <p class="description">
      Both renders originate from a single drone capture of Gasson Hall.  
      The first follows a classical photogrammetric workflow using Metashape;  
      the second applies a neural reconstruction via Nerfstudio (NeRF).  
      Together, they illustrate a spectrum of 3D modeling—from geometry-based  
      to learned representations—in an ongoing exploration of architectural-scale visualization.
    </p>

    <hr style="border: none; height: 1px; background: rgba(255,255,255,0.1); margin: 2rem 0;">

    <div class="video-block">
      <p class="tag">Classical Pipeline</p>
      <h2>Metashape Render</h2>
      <iframe width="560" height="315" src="https://www.youtube.com/embed/FWhEO5AJs8I?si=1oSppfNgDeR9N6VU" title="Metashape Render" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      <p class="video-caption">
        <strong>Captured</strong> with a DJI Matrice 4 Enterprise.<br>
        <strong>Processed</strong> in Metashape on a <strong>cloud H100 GPU</strong>: image alignment, scene cropping, dense cloud generation, mesh reconstruction, and <strong>UV-based texture mapping</strong> using the standard photogrammetry pipeline.<br>
        <strong>Flythrough</strong> authored and rendered in Metashape, exported to Blender for logo overlay and final rendering.
      </p>
    </div>

    <div class="video-block">
      <p class="tag">Neural Reconstruction</p>
      <h2>Nerfstudio Render</h2>
      <iframe width="560" height="315" src="https://www.youtube.com/embed/rxRdvqwOEvo?si=jcPH17tNypgAUQ4n" title="Nerfstudio Render" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
      <p class="video-caption">
        <strong>Input</strong> data and camera poses exported from Metashape for consistency.<br>
        <strong>Processed</strong> in Nerfstudio with nerfacto neural reconstruction trained on a <strong>cloud H100 GPU</strong>.<br>
        <strong>Flythrough</strong> authored and rendered in Nerfstudio, exported to Blender for logo overlay and final rendering.
      </p>
    </div>
  </section>

  <footer>
    <p>&copy; 2025 Caudell Spatial Intelligence</p>
  </footer>
</body>
</html>
